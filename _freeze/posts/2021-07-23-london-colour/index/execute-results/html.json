{
  "hash": "b0a91ad94a2d3d8bc1bec1fbebbd8422",
  "result": {
    "markdown": "---\ntitle: What colour is London?\ndate: 2021-07-23\nslug: london-colour\ncategories:\n  - dataviz\n  - magick\n  - r\n  - rtweet\n  - sf\n  - twitter\n---\n\n::: {.cell}\n\n:::\n\n\n![](resources/mosaic-random-1.png){fig-alt=\"A 25 by 25 grid of squares, each of which represents a random satellite image of Greater London that's been quantized to show one representative colour. This has resulted in various shades of grey, green and cream, depending on factors like urbanness and green space in the original satellite image. The arrangement of colours appears to be random.\" width=\"25%\"}\n\n## tl;dr\n\nI used the {rtweet} and {magick} R packages to fetch tweets of random satellite images of London from [londonmapbot](https://twitter.com/londonmapbot) and then reduced each one to a single representative colour.\n\n<div class=\"tip\"> ℹ️ <b>Update</b>\n\nlondonmapbot no longer posts to Twitter due to API changes. It can be found on Mastodon instead at [botsin.space/@londonmapbot](https://botsin.space/@londonmapbot). You can read about that in [a more recent post](https://www.rostrum.blog/2023/02/09/londonmapbotstodon/).\n\n<div>\n\n## Green/grey belt\n\nI created the [\\@londonmapbot](https://twitter.com/londonmapbot) Twitter bot to tweet out satellite images of random points in Greater London. You can read earlier posts about [how it was made](https://www.rostrum.blog/2020/09/21/londonmapbot/) and how I [mapped the points interactively](https://www.rostrum.blog/2020/12/20/londonmapbot-leaflet/).\n\nI figured we could sample these to get to 'the colours of London', which can be mapped or tiled.\n\nThis is not too dissimilar to efforts to find the 'average colour' of countries of the world, which [Erin wrote a nice post about](https://twitter.com/erindataviz), for example.[^erin] The difference is that we aren't finding a colour to represent London, we're representing London with a series of single-colour points. \n\nThis is relatively trivial with the packages [{rtweet}](https://docs.ropensci.org/rtweet/) to pull tweets and [{magick}](https://cran.r-project.org/web/packages/magick/vignettes/intro.html) to manipulate the images. We can use [{sf}](https://r-spatial.github.io/sf/) to place the points on a map and [{ggplot2}](https://ggplot2.tidyverse.org/) for other visualisations.\n\n## Get bot log\n\nFirst, load the packages we need. You'll need to use `install.packages()` for each one if you haven't already installed them.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsuppressPackageStartupMessages({\n  library(rtweet)\n  library(magick)\n  library(tidyverse)\n  library(sf)\n})\n```\n:::\n\n\n[{rtweet}](https://docs.ropensci.org/rtweet/) makes it very easy to collect tweet content. To the `get_timeline()` function you can pass an account name and the number of tweets you want to fetch. You'll need to [set up authenication first, of course](https://docs.ropensci.org/rtweet/articles/auth.html).\n\n\n::: {.cell hash='index_cache/html/get-tweets_685983ed616578365f40fc15a24c129f'}\n\n```{.r .cell-code}\ntweets_read <- get_timeline(\"londonmapbot\", n = 625)\n```\n:::\n\n\nWhy do I want 625? Well, the bot has tweeted out nearly 9000 images at time of writing, but I want a useable number for this post. (Spoiler: I also want to make a 25 by 25 grid of squares as one of my outputs.)\n\nThe function [actually returns more](https://github.com/ropensci/rtweet/issues/60) than 625 because {rtweet} maximises the number of tweets it fetches for each API call. Better to return more than you asked for, rather than less.\n\nThe returned tibble contains a lot of information. I'm only interested in the `media_url` and `text` columns, from which I can extract the satellite image URLs and, with some regular expressions, the coordinate information that's provided in the body of the tweet.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets <- tweets_read %>% \n  transmute(media_url = unlist(media_url), text) %>% \n  transmute(\n    media_url,\n    latitude = str_extract(text, \"^\\\\d{2}.\\\\d{1,4}\"),\n    longitude = str_extract(text, \"(?<=, ).*(?=\\nhttps)\")\n  ) %>% \n  slice(1:625)\n```\n:::\n\n\nSo we've got a tibble with 3 columns and 625 rows.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(tweets)\n```\n:::\n\n```\nRows: 625\nColumns: 3\n$ media_url <chr> \"http://pbs.twimg.com/media/E7BOglJVgAEE3XL.jpg\", \"http://pb…\n$ latitude  <chr> \"51.5651\", \"51.4665\", \"51.3752\", \"51.5041\", \"51.5668\", \"51.3…\n$ longitude <chr> \"0.0466\", \"-0.3526\", \"-0.1997\", \"-0.0174\", \"-0.1882\", \"-0.13…\n```\n\nI'm going to iterate through each URL to download the associated image to a temporary directory. I've used a `walk()` function from {purrr} rather than `map()` because we aren't returning anything; we're saving a file to a folder.\n\nSpecifically, I used `walk2()`, which lets me supply two values to the iterate process: the URL and also the iteration number for that URL. That means I can print a message in the form 'Fetching 1 of 625' and get a rough idea of progress.\n\nI've also added a `Sys.sleep()` call to slow the process, as not to hammer the Twitter API too hard.[^api]\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function: download images from URLs\ndownload_images <- function(paths, dir) {\n  \n  Sys.sleep(sample(0:2, 1))  # random pause\n  \n  tw_df <- data.frame(number = 1:length(paths), url = paths)\n  \n  purrr::walk2(\n    tw_df$number, tw_df$url, \n    ~ { cat(\"Fetching\", .x, \"of\", length(tw_df$number), \"\\n\")\n      download.file(.y, file.path(dir, basename(.y))) }\n  )\n  \n}\n```\n:::\n\n\nSo, you can pass a vector of URLs and a directory path to the function. For purposes of this post, I'm going to save the files to a temporary folder. \n\n\n\n\n\nThat call takes a little while and the duration will vary given the random pauses built into the function. I've hidden the output because there would be 625 items printed to the console. An example of the output:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nFetching 479 of 625 \ntrying URL 'http://pbs.twimg.com/media/E6Akw2fXMAA3VSk.jpg'\nContent type 'image/jpeg' length 113537 bytes (110 KB)\n==================================================\n  downloaded 110 KB\n```\n:::\n\n\nTo prove this has worked, we can fetch all the image paths from the directory in which they're stored and count how many there are.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nfiles <- list.files(tmp, \".jpg$\", full.names = TRUE)\nlength(files)\n```\n:::\n\n```\n[1] 625\n```\n\nGreat, as expected. Now we have a set of satellite images that we can manipulate.\n\n## Demo: one image\n\nAs a demo, let's take a look at the first image.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex_in <- image_read(files[1])\nex_in\n```\n:::\n\n\n![](resources/example-read-1.png){fig-alt=\"A satellite image of a ranom part of Greater London.\"}\n\nNow we can crop out the logos, reduce its colours and resize it using functions from [the {magick} package](https://cran.r-project.org/web/packages/magick/vignettes/intro.html). \n\n'Quantization' is the process we'll use on each image; it's basically [the algorithmic simplification of an image to the colours that best represent it](https://en.wikipedia.org/wiki/Color_quantization). You could, for example, use this for reducing the number of colours in an image to make it easier to compress while minimising information loss. We're going to quantize to just one colour to find the colour that best represents the image. Note that this isn't the same as 'taking an average colour'.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex_square <- ex_in %>%\n  image_crop(\"x420-0\") %>%\n  image_quantize(1) %>% \n  image_resize(\"100x100!\")\n\nex_square\n```\n:::\n\n\n![](resources/example-quantize-1.png){fig-alt=\"A square filled with a single colour that represents the satellite image og Greater London in the previous image in this blog post.\"}\n\nSo the colour of that square is what you get when you quantize the original satellite image down to one colour. What is that colour? We can extract the hex code.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex_rgb <- image_data(ex_square, channels = \"rgb\")[1:3]\nex_hex <- toupper(paste0(\"#\", paste(as.character(ex_rgb), collapse = \"\")))\nex_hex\n```\n:::\n\n```\n[1] \"#48503E\"\n```\n\nOf course, we can generally expect that the colour will be somewhere between very green (city fringes, parks, golf courses) and very grey (urban), while some may be more blue (reservoirs).\n\n## All images\n\nThe `image_*()` functions in {magick} are generally vectorised, so we can pass it all of the paths to our files and apply the wrangling steps across all of the images at once.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimgs_in <- image_read(files)\nimgs <- image_crop(imgs_in, \"x420-0\")\n```\n:::\n\n\nI want to grab the single quantized hex value representing each image.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nimgs_dat <- imgs %>% image_quantize(1) %>% image_resize(\"1x1!\")\nhex_dat <- map(1:625, ~image_data(imgs_dat, \"rgb\", frame = .x))\nhex_cols <- hex_dat %>% \n  map_chr(~paste0(\"#\", toupper(paste(.[1:3], collapse = \"\"))))\n\nhead(hex_cols)\n```\n:::\n\n```\n[1] \"#48503E\" \"#535C3F\" \"#435034\" \"#415534\" \"#5D6152\" \"#535F44\"\n```\n\nNow we can bind these to our tweets dataset.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets_cols <- tweets %>% bind_cols(hex = hex_cols)\nglimpse(tweets_cols)\n```\n:::\n\n```\nRows: 625\nColumns: 4\n$ media_url <chr> \"http://pbs.twimg.com/media/E7BOglJVgAEE3XL.jpg\", \"http://pb…\n$ latitude  <chr> \"51.5651\", \"51.4665\", \"51.3752\", \"51.5041\", \"51.5668\", \"51.3…\n$ longitude <chr> \"0.0466\", \"-0.3526\", \"-0.1997\", \"-0.0174\", \"-0.1882\", \"-0.13…\n$ hex       <chr> \"#48503E\", \"#535C3F\", \"#435034\", \"#415534\", \"#5D6152\", \"#535…\n```\n\n### Visualisation: map\n\nThe obvious thing to do is to create a map with each point marking the location of a satellite image tweeted by [londonmapbot](https://twitter.com/londonmapbot), filled with the single representative colour for that image.\n\nThe bot samples from a square roughly covering Greater London within the M25, so it might be nice to show the outline of London for reference. [The {sf} package](https://r-spatial.github.io/sf/) makes it straightforward to read [a GeoJSON of the NUTS1 boundaries](https://geoportal.statistics.gov.uk/search?collection=Dataset&sort=name&tags=all(BDY_NUTS1%2CJAN_2018)) for the UK via [the Open Geography Portal](https://geoportal.statistics.gov.uk/) API, then convert it to latitude-longitude coordinates and filter for London only.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnuts_path <- \"https://opendata.arcgis.com/datasets/01fd6b2d7600446d8af768005992f76a_4.geojson\"\nldn_sf <- st_read(nuts_path) %>% \n  st_transform(crs = 4326) %>%\n  filter(nuts118nm == \"London\")\n```\n:::\n\n```\nReading layer `NUTS_Level_1_(January_2018)_Boundaries' from data source `https://opendata.arcgis.com/datasets/01fd6b2d7600446d8af768005992f76a_4.geojson' using driver `GeoJSON'\nSimple feature collection with 12 features and 9 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -8.649996 ymin: 49.88234 xmax: 1.762942 ymax: 60.86078\nGeodetic CRS:  WGS 84\n```\n\nAnd we can convert our tweets tibble to an sf-class spatial object as well, given that it contains coordinate information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntweets_sf <- tweets_cols %>% \n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326)\n```\n:::\n\n\nThen it's a case of adding these to a map, which in this case is a {ggplot2} object. The `geom_sf()` function is great at accepting and understanding polygons and points.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot() +\n  geom_sf(data = tweets_sf, col = hex_cols, size = 3) +\n  geom_sf(data = ldn_sf, alpha = 0, size = 1, col = \"black\") +\n  theme_void()\n```\n:::\n\n\n![](resources/map-sf-1.png){fig-alt=\"Random points are arranged over a simplified boundary of greater London. Each point represents the location for which the londonmapbot Twitter accoutn tweeted a satellite image. The points are various shades of green through grey, with the colour representing the image via a process of quantization.\" fig-align=\"left\"}\n\nAre there any patterns here? Maybe it's greener in the suburbs? (It's a serious question; I'm a deuteranope.)[^deuteranope]\n\n### Visualisation: tiles\n\nRecently I've written some posts involving R and abstract art (like [pixel art](https://www.rostrum.blog/2021/06/28/pixel-art/) and [a Shiny app to remix art by Sol LeWitt](https://www.rostrum.blog/2021/07/05/recreate-lewitt/)).\n\nSo why not get more abstract with these data points? We can create squares of each colour and tile them. \n\nHere the tiles are laid out row by row from right to left, in a more-or-less random order.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhex_tiles <- crossing(x = 1:25, y = 1:25) %>% \n  bind_cols(hex = tweets_cols$hex)\n\nggplot() +\n  geom_tile(aes(hex_tiles$x, hex_tiles$y), fill = hex_tiles$hex) +\n  theme_void()\n```\n:::\n\n\n![](resources/mosaic-random-1.png){fig-alt=\"A 25 by 25 grid of squares, each of which represents a random satellite image of Greater London that's been quantized to show one representative colour. This has resulted in various shades of grey, green and cream, depending on factors like urbanness and green space in the original satellite image. The arrangement of colours appears to be random.\" fig-align=\"left\"}\n\nFor fans of order, we could instead arrange them by brightness, or 'luminance'.[^hard] Here I've modified a simple approach by [Wouter van der Bijl](https://www.woutervdbijl.com/) from [a StackOverflow post](https://stackoverflow.com/questions/61193516/how-to-sort-colours-in-r).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Get luminance for hex values\nrgb_vals <- col2rgb(tweets_cols$hex)  # Hex to RGB\nlab_vals <- convertColor(t(rgb_vals), 'sRGB', 'Lab')  # RGB to Lab\nhex_lum <- tweets_cols$hex[order(lab_vals[, 'L'])]  # luminance order\n\n# Set up dummy xy tile locations\ncross_xy <- crossing(y = 1:25, x = 1:25)\n\n# Create tibble of x, y, hex luminance\nhex_tiles_bright <- tibble(\n  x = cross_xy$x,\n  y = rev(cross_xy$y),\n  hex = hex_lum\n)\n\n# Plot so 'lightest' in top left, 'darkest' in bottom right\nggplot(hex_tiles_bright) +\n  geom_tile(aes(x, y), fill = rev(hex_tiles_bright$hex)) +\n  theme_void()\n```\n:::\n\n\n![](resources/mosaic-col-1.png){fig-alt=\"A 25 by 25 grid of squares, each of which represents a random satellite image of Greater London that's been quantized to show one representative colour. This has resulted in various shades of grey, green and cream, depending on factors like urbanness and green space in the original satellite image. The squares are ordered form brightest in the top-left to darkest in the lower-right.\" fig-align=\"left\"}\n\nThe colours make me think of the classic smoggy ['pea souper'](https://en.wiktionary.org/wiki/pea-souper) of London in times past, which is fitting.\n\nOr, y'know, sewage.\n\nOf course, there's lots more images available in [the londonmapbot feed](https://twitter.com/londonmapbot) and many other ways to visualise these data, so I may return to this idea in the future.\n\n## Environment {.appendix}\n\n<details><summary>Session info</summary>\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nLast rendered: 2023-07-09 13:24:49 BST\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.23    knitr_1.43.1      jsonlite_1.8.7    xfun_0.39        \n[13] digest_0.6.31     rlang_1.1.1       evaluate_0.21    \n```\n:::\n:::\n\n</details>\n\n[^api]: I'm being relatively [polite](https://www.rostrum.blog/2019/03/04/polite-webscrape/) by doing this, it's probably not strictly necessary.\n[^erin]: I didn't find Erin's post until after starting my post, but I see that there are similarities in tools: Erin makes use of many of the {magick} functions that I have, for example. This makes me think I've used a sensible approach.\n[^hard]: Colour is a hard concept and the idea of 'brightness' is no exception. We're keeping things naïve here.\n[^deuteranope]: As in the ['nope' to green/brown/red](https://en.wikipedia.org/wiki/Color_blindness) sort of colourblindness.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}