{
  "hash": "81ac6d1f76225d9db0815b2a6d167239",
  "result": {
    "markdown": "---\ntitle: \"Decay is inevitable, accept {linkrot}?\"\ndate: 2021-07-10\nslug: linkrot\ncategories:\n  - httr\n  - linkrot\n  - r\n  - rvest\n  - xml2\n  - webshot\n---\n\n\n\n\n![](resources/404.png){fig-alt=\"A screenshot of a 404 page, which is reached by following a broken link. The text says 'page not found, looks like you've followed a broken link or entered a URL that doesn't exist on this site'.\" width=\"100%\"}\n\n## tl;dr\n\nI wrote a little function to check web pages for [link rot](https://en.wikipedia.org/wiki/Link_rot) and put it in [the tiny R package {linkrot}](https://github.com/matt-dray/linkrot) in case you want to use or improve it.\n\n## Page not found\n\nYou've clicked a link before and been taken somewhere you weren't expecting. Sometimes it's because [you've been rickrolled](https://pudding.cool/2021/07/rickrolling/),[^rick] sure, but content on the internet is constantly being moved or removed and links break all the time.\n\nA hyperlink that no longer resolves can be considered to have 'rotted'. As time marches on, the 'rottenness' of the internet increases. This can be frustrating.\n\nThis blog is getting on for a hundred posts over three years. It would not be a surprise if link rot has taken hold. How big is the problem?\n\n## Rising damp\n\nSo, basically I want to visit every link in every post on this blog and see if it's still working.[^link]\n\nI've written the function `detect_rot()` to do this for any given web page and I've put it in [the {linkrot} package on GitHub](https://github.com/matt-dray/linkrot). To install:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ninstall.packages(\"remotes\")  # if not yet installed\nremotes::install_github(\"matt-dray/linkrot\")\nlibrary(linkrot)\n```\n:::\n\n\nIn short, the `detect_rot()` function takes the URL of a web page and returns a tibble with details of each link from that page and whether it can be reached.\n\nI've basically built it for my own amusement, so there's no guarantees. Feel free to suggest or amend things in [the GitHub repo](https://github.com/matt-dray/linkrot).\n\n### Check one post\n\nLet's feed in the first post on this blog, from April 2018:\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrek_url <- \"https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/\"\ntrek_rot <- detect_rot(trek_url)\n```\n:::\n\n```\nChecking <https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/> ..............................\n```\n\nIt can take a short while for the function to visit every link. To let us know it's working, the URL is printed to the console and then a period (`.`) is printed for every link that's been successfully visited (a bit like a progress bar).\n\nWe're returned an object with a bunch of information.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(trek_rot)\n```\n:::\n\n```\ntibble [30 × 6] (S3: tbl_df/tbl/data.frame)\n $ page             : chr [1:30] \"https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/\" \"https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/\" \"https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/\" \"https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/\" ...\n $ link_url         : chr [1:30] \"https://www.r-project.org/about.html\" \"https://en.wikipedia.org/wiki/Star_Trek:_The_Next_Generation\" \"http://www.st-minutiae.com/resources/scripts/#thenextgeneration\" \"https://github.com/zeeshanu/learn-regex/blob/master/README.md\" ...\n $ link_text        : chr [1:30] \"R statistical software\" \"Star Trek: The Next Generation\" \"Star Trek Minutiae\" \"regex\" ...\n $ response_code    : num [1:30] 200 200 200 200 200 200 200 404 200 200 ...\n $ response_category: chr [1:30] \"Success\" \"Success\" \"Success\" \"Success\" ...\n $ response_success : logi [1:30] TRUE TRUE TRUE TRUE TRUE TRUE ...\n```\n\nSo, it's a tibble with six columns and a row for each link on that page that's been checked. Basically, the output tells us the URL and text of each link and also whether the page was reachable or not. \n\nThe tibble includes a special officially-standardised three-digit ['status code'](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) in the `response_code` column. These indicate whether contact was successful, with a specific reason. For example, 200 represents a typical success ('OK'), but you may be familiar with 404 ('not found') if you've visited a broken link before.\n\nWe can extract any broken links using the logical `response_success` column.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ntrek_rot[!trek_rot$response_success, c(4, 5, 2)]\n```\n:::\n\n```\n# A tibble: 1 x 3\n  response_code response_category link_url                                      \n          <dbl> <chr>             <chr>                                         \n1           404 Client error      https://cran.r-project.org/web/packages/rvest…\n```\n\nSo, at time of writing, that post has one broken link: an {rvest} package vignette for SelectorGadget that's no longer active on the CRAN site. It has [status code 404 ('client error')](https://en.wikipedia.org/wiki/HTTP_404), which basically means the thing couldn't be found.\n\nWe can confirm this by [visiting the URL](https://cran.r-project.org/web/packages/rvest/vignettes/selectorgadget.html), but you could also use [the {webshot} package](https://wch.github.io/webshot/articles/intro.html) to go and retrieve an screenshot of the page[^phantomjs].\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(webshot)\ncran_404 <- trek_rot$link_url[!trek_rot$response_success]\nwebshot(cran_404, vheight = 250)\n```\n:::\n\n\n![](resources/webshot-404.png){fig-alt=\"A screenshot of the simple 404 page for the CRAN website, which can be seen when trying to access a URL that has no content. It says 'object not found' and that 'the requested URL was not found on this server.\" width=\"100%\"}\n\nSo that's CRAN's 404 page to tell us that the page couldn't be fetched.\n\n### Check whole blog\n\nNow we know how it works for one page, we can apply the function over every post of this blog and see how many links have rotted. \n\nFirst we need all the post URLs, which are all available from [the blog's homepage](https://www.rostrum.blog). The links returned are internal (like `2021/06/28/pixel-art/`), so we need to add on the `https://www.rostrum.blog/` bit. We also need to filter out any links that aren't posts (like the 'About' page).\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load packages\nsuppressPackageStartupMessages({\n  library(xml2)\n  library(rvest)\n  library(dplyr)\n  library(purrr)\n})\n\n# The URL of this blog's homepage\nblog_url <- \"https://www.rostrum.blog\"\n\n# Fetch all the links from the blog home page\nblog_hrefs <- \n  read_html(blog_url) %>%  # get full homepage HTML\n  html_nodes(\"a\") %>%      # nodes with links, <a>\n  html_attr(\"href\")        # the URL attribute\n\n# Only links to posts\nposts <- paste0(blog_url, blog_hrefs[grepl(\"^/20\", blog_hrefs)])\ntail(posts)  # preview\n```\n:::\n\n```\n[1] \"https://www.rostrum.blog/2018/06/05/tid-ye-text/\"                             \n[2] \"https://www.rostrum.blog/2018/05/25/cloud-pie/\"                               \n[3] \"https://www.rostrum.blog/2018/05/19/pokeballs-in-super-smash-bros/\"           \n[4] \"https://www.rostrum.blog/2018/05/12/accessibility-workshop-at-sprint18/\"      \n[5] \"https://www.rostrum.blog/2018/04/27/two-dogs-in-toilet-elderly-lady-involved/\"\n[6] \"https://www.rostrum.blog/2018/04/14/r-trek-exploring-stardates/\"\n```\n\nNow we can use {purrr} to iterate the `detect_rot()` function over the pages. By using `map_df()` we can get a data frame as output rather than a list. I've hidden the printed output from `detect_rot()` this time because there would be nearly 100 lines of output (one per post).\n\n\n::: {.cell hash='index_cache/html/map-df_8f137333ffe851c17441350b7557a7f1'}\n\n```{.r .cell-code}\nresults <- map_df(posts, detect_rot)\n```\n:::\n\n\nSo, this results tibble has 23311 links from 95 posts, or about 25 links per post.\n\nAgain, we can filter the logical `response_success` column to see which links weren't successfully resolved.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrotten <- filter(results, !response_success)\nnrow(rotten)\n```\n:::\n\n```\n[1] 61\n```\n\nSo in total there were 61 links out of 2331 that did not return a 'success', which works out to about 3% being unreachable.\n\nWe can count the reasons for these failures by looking at the status codes.\n\n\n::: {.cell}\n\n```{.r .cell-code}\ncount(rotten, response_code, sort = TRUE)\n```\n:::\n\n```\n# A tibble: 6 x 2\n  response_code     n\n          <dbl> <int>\n1           404    53\n2           400     4\n3           403     1\n4           406     1\n5           410     1\n6           502     1\n```\n\nYou can see most of these status codes are in [the 4xx range](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes#4xx_client_errors), which is the group of codes that mean 'client error'. Usually this is a problem with the link you've provided, like 404 is 'not found', 403 is 'forbidden' and 406 is 'not acceptable'.\n\nIt's hard to tell whether this level of link rot is good or bad, but remember that these are links that have failed within the past three years. Imagine how bad this might look in another 10 years. By comparison, a quarter of links on the _New York Times_ website [were completely inaccessible](https://www.cjr.org/analysis/linkrot-content-drift-new-york-times.php), stretching back to 1996.\n\nI'd be interested to know whether this is comparable to your blog or website.\n\n## Surveying for rot\n\nWe've seen it in action, but how does the function work? I'm not claiming the approach is optimal, but it obviously worked for my needs. You'll probably find the approach naive if you have any experience in dealing with HTTP requests from R.\n\n### Validate, fetch, check\n\nYou can find the function definition for `detect_rot()` [in the {linkrot} source code](https://github.com/matt-dray/linkrot/blob/main/R/check.R). It has three underlying steps, each of which has [a helper function](https://github.com/matt-dray/linkrot/blob/main/R/utils.R):\n\n1. Check that the provided URL is valid with `.validate_page()`\n1. Scrape the links from the page with `.fetch_links()`\n1. Visit each link and check its response code with `.check_links()`\n\nSo, the URL provided by the user is first checked with help from the {httr} package. We `GET()` the page and then extract the `status_code()` and check for an `http_error()`. If all is well (i.e. no error), then we can continue.\n\nTo get the links from the URL, we first scrape the page with `xml2::read_html()` and then use {rvest} functions: `html_nodes()` to grab all the nodes with links, then `html_attr()` and `html_text()` to extract the URLs and link text from each.\n\nFinally, each of the URLs is visited with `GET()` and the `http_status()` is extracted. The final data frame is converted to tibble (for ease of reading) and returned to the user.\n\n### Limitations\n\nOf course, it's possible that `GET()` will fail to reach a page for reasons other than it being missing. Sometimes there can be a momentary blip, but `detect_rot()` is simple and never retries a link.\n\nAdditionally, there are some links that {httr} struggles to contact. I wrapped functions internal to `detect_rot()` inside `tryCatch()` so any failures appear as `NA` in the `response_code` column. The printed output for `detect_rot()` also displays an exclamation point (`!`) instead of a period (`.`) when being run. For example, there were 8 links that had this problem for this blog.\n\nI welcome any thoughts or suggestions, particularly around testing. I'd like to use this package as a way to learn proper HTTP testing and have found [rOpenSci's _HTTP Testing in R_ book](https://books.ropensci.org/http-testing/index.html) useful so far. Eventually I might convert `detect_rot()` to use [the {httr2} package](https://httr2.r-lib.org/) when it's released.\n\n## Now what?\n\nI could go back and fix the broken links, but maybe it's not that big a deal. I don't have any data on what people click on, so I can't really tell if it's worth it.\n\nBut anyway, didn't I say 'decay is inevitable'? I can fix things, but more things will break.\n\nI wasn't expecting this to get quite so existential.[^spoiler]\n\n## Environment {.appendix}\n\n<details><summary>Session info</summary>\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nLast rendered: 2023-07-09 19:23:02 BST\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.5   rstudioapi_0.14   yaml_2.3.7       \n [9] rmarkdown_2.23    knitr_1.43.1      jsonlite_1.8.7    xfun_0.39        \n[13] digest_0.6.31     rlang_1.1.1       evaluate_0.21    \n```\n:::\n:::\n\n</details>\n\n[^rick]: Don't worry, you can see from the URL that this doesn't go to the YouTube video! it goes to the excellent pudding.cool site, which has some great analysis of the rise and rise (and rise) of Rickrolling.\n[^link]: That's links to other pages on the internet, because links also exist to take you to these footnotes, or point elsewhere internally to this website.\n[^spoiler]: Actually yes, dear reader, he was; he really was.\n[^phantomjs]: If not already installed, you'll be prompted by {webshot} to install phantomjs with `webshot::install_phantomjs()`.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}