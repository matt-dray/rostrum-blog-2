{
  "hash": "22299a5b109fad3e26e88c8c6c72fffe",
  "result": {
    "markdown": "---\ntitle: Web scraping the {polite} way\ndate: 2019-03-04\nslug: polite-webscrape\ncategories:\n  - polite\n  - r\n  - rvest\n  - tidyverse\n  - webscrape\n---\n\n::: {.cell}\n\n:::\n\n\n![](resources/tea.gif){fig-alt=\"Martin Freeman as Douglas Adams's Arthur Dent, taking a sip of tea and saying 'oh, come on, that's lovely'\" width=\"50%\" fig-align=\"left\"}\n\n## tl;dr\n\nIf you webscrape with R, you should use the [the {polite} package](https://www.github.com/dmi3kno/polite/). It helps you respect website terms by seeking permission before you scrape.\n\n## Ahoy-hoy\n\nAh, salutations, and welcome to this blog post about polite web scraping. Please do come in. I'll take your coat. How are you? Would you like a cup of tea? Oh, I insist!\n\nSpeaking of tea, perhaps you'd care to join me in genial conversation about it. Where to begin? Let's draw inspiration from popular posts on [the Tea subreddit of Reddit](https://www.reddit.com/r/tea/). I'll fetch the post titles using [the {rvest} package](https://rvest.tidyverse.org/) from Hadley Wickham and get the correct CSS selector using [SelectorGadget](https://selectorgadget.com/) by Andrew Cantino and Kyle Maxwell.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load some packages we need\nlibrary(rvest)  # scrape a site\nlibrary(dplyr)  # data manipulation\n\n# CSS for post titles found using SelectorGadget\n# (This is a bit of an odd one)\ncss_select <- \"._3wqmjmv3tb_k-PROt7qFZe ._eYtD2XCVieq6emjKBH3m\"\n\n# Scrape a specific named page\ntea_scrape <- read_html(\"https://www.reddit.com/r/tea\") %>%  # read the page\n  html_nodes(css = css_select) %>%  # read post titles\n  html_text()  # convert to text\n\nprint(tea_scrape)\n```\n:::\n\n```\n[1] \"What's in your cup? Daily discussion, questions and stories - September 08, 2019\"                                                                 \n[2] \"Marketing Monday! - September 02, 2019\"                                                                                                           \n[3] \"Uncle Iroh asking the big questions.\"                                                                                                             \n[4] \"The officially licensed browser game of Game of Thrones has launched! Millions of fans have put themselves into the battlefield! What about you?\" \n[5] \"They mocked me. They said that I was a fool for drinking leaf water.\"                                                                             \n[6] \"100 years old tea bush on my estate in Uganda.\"                                                                                                   \n[7] \"Cold brew colors\"                                                                                                                                 \n[8] \"Finally completed the interior of my tea house only needed a fire minor touches not now itâ€™s perfect, so excited to have this as a daily tea spot\"\n```\n\nThat'll provide us with some conversational fodder, wot wot.\n\n## It costs nothing to be polite\n\nMercy! I failed to doff adequately my cap before entering the website! They must take me for some sort of street urchin.\n\nForgive me. Perhaps you'll allow me to show you a more respectful method via [the {polite} package](https://www.github.com/dmi3kno/polite/) in development from the esteemed gentleman [Dmytro Perepolkin](https://twitter.com/dmi3k)? An excellent way 'to promote responsible web etiquette'.\n\n## A reverential bow()\n\nPerhaps the website owners don't want people to keep barging in willy-nilly without so much as a 'ahoy-hoy'.\n\nWe should identify ourselves and our intent with a humble `bow()`. We can expect a curt but informative response from the site---via its [robots.txt file](http://www.robotstxt.org/robotstxt.html)---that tells us where we can visit and how frequently.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# remotes::install_github(\"dmi3kno/polite\")  # to install\nlibrary(polite)  # respectful webscraping\n\n# Make our intentions known to the website\nreddit_bow <- bow(\n  url = \"https://www.reddit.com/\",  # base URL\n  user_agent = \"M Dray <https://rostrum.blog>\",  # identify ourselves\n  force = TRUE\n)\n\nprint(reddit_bow)\n```\n:::\n\n```\n## <polite session> https://www.reddit.com/\n##      User-agent: M Dray <https://rostrum.blog>\n##      robots.txt: 32 rules are defined for 4 bots\n##     Crawl delay: 5 sec\n##   The path is scrapable for this user-agent\n```\n\nSuper-duper. The (literal) bottom line is that we're allowed to scrape. The website does have 32 rules to stop unruly behaviour though, and even calls out four very naughty bots that are obviously not very polite. We're invited to give a five-second delay between requests to allow for maximum respect.\n\n## Give a nod()\n\nAhem, conversation appears to be wearing a little thin; perhaps I can interest you by widening the remit of our chitchat? Rather than merely iterating though subpages of the same subreddit, we can visit the front pages of a few different subreddits. Let's celebrate the small failures and triumphs of being British; a classic topic of polite conversation in Britain.\n\nWe've already given a `bow()` and made out intentions clear; a knowing `nod()` will be sufficient for the next steps. Here's a little function to `nod()` to the site each time we iterate over a vector of subreddit names. Our gentlemanly agreement remains intact from our earlier `bow()`.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(purrr)  # functional programming tools\nlibrary(tidyr)  # tidy-up data structure\n\nget_posts <- function(subreddit_name, bow = reddit_bow, css_select){\n  \n  # 1. Agree modification of session path with host\n  session <- nod(\n    bow = bow,\n    path = paste0(\"r/\", subreddit_name)\n  )\n  \n  # 2. Scrape the page from the new path\n  scraped_page <- scrape(session)\n  \n  # 3. Extract from xpath on the altered URL\n  node_result <- html_nodes(\n    scraped_page,\n    css = css_select\n  )\n  \n  # 4. Render result as text\n  text_result <- html_text(node_result)\n  \n  # 5. Return the text value\n  return(text_result)\n  \n}\n```\n:::\n\n\nSmashing. Care to join me in applying this function over a vector of subreddit names? Tally ho.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# A vector of subreddits to iterate over\nsubreddits <- set_names(c(\"BritishProblems\", \"BritishSuccess\"))\n\n# Get top posts for named subreddits\ntop_posts <- map_df(\n  subreddits,\n  ~get_posts(.x, css_select = \"._3wqmjmv3tb_k-PROt7qFZe ._eYtD2XCVieq6emjKBH3m\")\n) %>% \n  gather(\n    BritishProblems, BritishSuccess,\n    key = subreddit, value = post_text\n  )\n\nknitr::kable(top_posts)\n```\n:::\n\n\nBravo, what excellent manners we've demonstrated. You can also iterate over different query strings -- for example if your target website displays information over several subpages -- with the `params` argument of the `scrape()` function.\n\nOh, you have to leave? No, no, you haven't overstayed your welcome! It was truly marvellous to see you. Don't forget your brolly, old chap, and don't forget to print the session info for this post. Pip pip!\n\n## Environment {.appendix}\n\n<details><summary>Session info</summary>\n\n::: {.cell}\n::: {.cell-output .cell-output-stdout}\n```\nLast rendered: 2023-08-02 23:36:13 BST\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nR version 4.3.1 (2023-06-16)\nPlatform: aarch64-apple-darwin20 (64-bit)\nRunning under: macOS Ventura 13.2.1\n\nMatrix products: default\nBLAS:   /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRblas.0.dylib \nLAPACK: /Library/Frameworks/R.framework/Versions/4.3-arm64/Resources/lib/libRlapack.dylib;  LAPACK version 3.11.0\n\nlocale:\n[1] en_US.UTF-8/en_US.UTF-8/en_US.UTF-8/C/en_US.UTF-8/en_US.UTF-8\n\ntime zone: Europe/London\ntzcode source: internal\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.2 compiler_4.3.1    fastmap_1.1.1     cli_3.6.1        \n [5] tools_4.3.1       htmltools_0.5.5   rstudioapi_0.15.0 yaml_2.3.7       \n [9] rmarkdown_2.23    knitr_1.43.1      jsonlite_1.8.7    xfun_0.39        \n[13] digest_0.6.33     rlang_1.1.1       evaluate_0.21    \n```\n:::\n:::\n\n</details>",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}